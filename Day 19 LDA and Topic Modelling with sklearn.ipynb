{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daeab9c3",
   "metadata": {},
   "source": [
    "# Create Dummy Textual Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05a577f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I love programming in Python',\n",
       " 'Python and Java are popular programming languages',\n",
       " 'I enjoy learning new programming languages',\n",
       " 'Machine learning is fascinating',\n",
       " 'Deep learning and neural networks are part of machine learning',\n",
       " 'Natural language processing (NLP) is a branch of artificial intelligence',\n",
       " 'NLP techniques include tokenization, stemming, and lemmatization',\n",
       " 'Supervised learning algorithms include regression and classification',\n",
       " 'Unsupervised learning includes clustering and association',\n",
       " 'Reinforcement learning involves agents learning from their environment']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Example documents\n",
    "documents = [\n",
    "    \"I love programming in Python\",\n",
    "    \"Python and Java are popular programming languages\",\n",
    "    \"I enjoy learning new programming languages\",\n",
    "    \"Machine learning is fascinating\",\n",
    "    \"Deep learning and neural networks are part of machine learning\",\n",
    "    \"Natural language processing (NLP) is a branch of artificial intelligence\",\n",
    "    \"NLP techniques include tokenization, stemming, and lemmatization\",\n",
    "    \"Supervised learning algorithms include regression and classification\",\n",
    "    \"Unsupervised learning includes clustering and association\",\n",
    "    \"Reinforcement learning involves agents learning from their environment\"\n",
    "]\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7707c21d",
   "metadata": {},
   "source": [
    "# Preprocessing (Cleaning Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1858d18f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['love programming python',\n",
       " 'python java popular programming languages',\n",
       " 'enjoy learning new programming languages',\n",
       " 'machine learning fascinating',\n",
       " 'deep learning neural networks machine learning',\n",
       " 'natural language processing nlp branch artificial intelligence',\n",
       " 'nlp techniques include tokenization stemming lemmatization',\n",
       " 'supervised learning algorithms include regression classification',\n",
       " 'unsupervised learning includes clustering association',\n",
       " 'reinforcement learning involves agents learning environment']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing Steps\n",
    "# Lowercasing: Convert all characters to lowercase.\n",
    "# Removing Punctuation: Remove punctuation marks.\n",
    "# Removing Stopwords: Remove common stopwords like \"and\", \"the\", etc.\n",
    "# Tokenization: Split text into individual words.\n",
    "# Stemming/Lemmatization: Reduce words to their root form (optional).\n",
    "\n",
    "#===============================================================================\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import re\n",
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Tokenize and remove stopwords\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in ENGLISH_STOP_WORDS]\n",
    "    # Join tokens back to string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Preprocess the documents\n",
    "preprocessed_documents = [preprocess_text(doc) for doc in documents]\n",
    "preprocessed_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d16f5e0",
   "metadata": {},
   "source": [
    "# Countvectorizer (Text to numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15419102",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Convert the documents to a term-document matrix\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(preprocessed_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1b00e72a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "         0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0,\n",
       "         1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "         0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "         0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0],\n",
       "        [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optional code \n",
    "# Convert the sparse matrix to a dense format\n",
    "dense_matrix = X.todense()\n",
    "dense_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b974512d",
   "metadata": {},
   "source": [
    "# Apply LDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f47399a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LatentDirichletAllocation(n_components=2, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LatentDirichletAllocation</label><div class=\"sk-toggleable__content\"><pre>LatentDirichletAllocation(n_components=2, random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LatentDirichletAllocation(n_components=2, random_state=0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "# Fit the LDA model\n",
    "lda = LatentDirichletAllocation(n_components=2, random_state=0)\n",
    "lda.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352125b2",
   "metadata": {},
   "source": [
    "# Display topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "53724bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "['learning', 'programming', 'languages', 'machine', 'python', 'neural', 'deep', 'networks', 'environment', 'involves']\n",
      "Topic 1:\n",
      "['nlp', 'include', 'processing', 'natural', 'branch', 'intelligence', 'artificial', 'language', 'stemming', 'techniques']\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in enumerate(lda.components_):\n",
    "    print(f\"Topic {idx}:\")\n",
    "    print([vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-10 - 1:-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dc8907",
   "metadata": {},
   "source": [
    "# let's make it more representative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6a0aea18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic 1</th>\n",
       "      <th>Topic 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>learning</td>\n",
       "      <td>nlp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>programming</td>\n",
       "      <td>include</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>languages</td>\n",
       "      <td>processing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>machine</td>\n",
       "      <td>natural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>python</td>\n",
       "      <td>branch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>neural</td>\n",
       "      <td>intelligence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>deep</td>\n",
       "      <td>artificial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>networks</td>\n",
       "      <td>language</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>environment</td>\n",
       "      <td>stemming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>involves</td>\n",
       "      <td>techniques</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Topic 1       Topic 2\n",
       "0     learning           nlp\n",
       "1  programming       include\n",
       "2    languages    processing\n",
       "3      machine       natural\n",
       "4       python        branch\n",
       "5       neural  intelligence\n",
       "6         deep    artificial\n",
       "7     networks      language\n",
       "8  environment      stemming\n",
       "9     involves    techniques"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    topic_dict = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topic_dict[f\"Topic {topic_idx+1}\"] = [feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "    return topic_dict\n",
    "\n",
    "no_top_words = 10\n",
    "topics = display_topics(lda, vectorizer.get_feature_names_out(), no_top_words)\n",
    "\n",
    "# Convert the topics dictionary to a DataFrame for better visualization\n",
    "topics_df = pd.DataFrame(topics)\n",
    "\n",
    "\n",
    "topics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8f35d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f48df7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41efb735",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
